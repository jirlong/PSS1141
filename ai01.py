from openai import OpenAI
from pydantic import BaseModel
import json

OPENAI_API_KEY = ""
client = OpenAI(api_key=OPENAI_API_KEY)


# å®šç¾© 5W1H çš„ JSON Schema
class News5W1H(BaseModel):
    who: str      # èª° - æ–°èä¸»è§’
    what: str     # ä»€éº¼äº‹ - ç™¼ç”Ÿçš„äº‹ä»¶
    when: str     # ä½•æ™‚ - æ™‚é–“
    where: str    # ä½•åœ° - åœ°é»
    why: str      # ç‚ºä½• - åŸå› 
    how: str      # å¦‚ä½• - æ–¹æ³•æˆ–éç¨‹


def ask(input_text, model="gpt-5-nano", instructions="You are a helpful assistant."):
    """
    é€šç”¨çš„å•ç­”å‡½å¼
    
    åƒæ•¸:
        input_text (str): è¼¸å…¥çš„å•é¡Œæˆ–æç¤º
        model (str): ä½¿ç”¨çš„æ¨¡å‹,é è¨­ç‚º "gpt-5-nano"
        instructions (str): ç³»çµ±æŒ‡ç¤º,é è¨­ç‚º "You are a helpful assistant."
    
    å›å‚³:
        str: AI çš„å›æ‡‰
    """
    response = client.responses.create(
        model=model,
        instructions=instructions,
        input=input_text
    )
    return response.output_text


def news_5w1h_summarizer(news_text, model="gpt-5-nano"):
    """
    æ–°è 5W1H æ‘˜è¦å‡½å¼
    å¾æ–°èå…§å®¹ä¸­æå– Who, What, When, Where, Why, How è³‡è¨Š
    
    åƒæ•¸:
        news_text (str): æ–°èå…§å®¹æ–‡å­—
        model (str): ä½¿ç”¨çš„æ¨¡å‹,é è¨­ç‚º "gpt-4o-2024-08-06" (æ”¯æ´ structured output)
    
    å›å‚³:
        dict: åŒ…å« 5W1H è³‡è¨Šçš„å­—å…¸
    """
    response = client.responses.parse(
        model=model,
        input=[
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ–°èåˆ†æå¸«ã€‚è«‹å¾æ–°èå…§å®¹ä¸­æå– 5W1H è³‡è¨Š:
- who: æ–°èä¸­çš„ä¸»è¦äººç‰©æˆ–çµ„ç¹”
- what: ç™¼ç”Ÿäº†ä»€éº¼äº‹ä»¶
- when: äº‹ä»¶ç™¼ç”Ÿçš„æ™‚é–“
- where: äº‹ä»¶ç™¼ç”Ÿçš„åœ°é»
- why: äº‹ä»¶ç™¼ç”Ÿçš„åŸå› æˆ–å‹•æ©Ÿ
- how: äº‹ä»¶å¦‚ä½•ç™¼ç”Ÿæˆ–åŸ·è¡Œçš„æ–¹å¼

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”,å¦‚æœæŸé …è³‡è¨Šåœ¨æ–°èä¸­æœªæåŠ,è«‹å¡«å¯«ã€ŒæœªæåŠã€ã€‚"""
            },
            {
                "role": "user",
                "content": news_text
            }
        ],
        text_format=News5W1H
    )
    
    # å–å¾—çµæ§‹åŒ–çš„è¼¸å‡º
    result = response.output_parsed
    return result.model_dump()


def translator(text, model="gpt-5-nano"):
    """
    æ™ºèƒ½ç¿»è­¯å‡½å¼:è‡ªå‹•åˆ¤æ–·èªè¨€ä¸¦ç¿»è­¯
    - è¼¸å…¥ç¹é«”ä¸­æ–‡ â†’ ç¿»è­¯æˆè‹±æ–‡
    - è¼¸å…¥è‹±æ–‡ â†’ ç¿»è­¯æˆç¹é«”ä¸­æ–‡
    
    åƒæ•¸:
        text (str): è¦ç¿»è­¯çš„æ–‡å­—
        model (str): ä½¿ç”¨çš„æ¨¡å‹,é è¨­ç‚º "gpt-5-nano"
    
    å›å‚³:
        str: ç¿»è­¯çµæœ
    """
    def _ask(text, model="gpt-5-nano"):
        instructions = """You are a professional translator.
        If the input text is in Traditional Chinese (ç¹é«”ä¸­æ–‡), translate it to English.
        If the input text is in English, translate it to Traditional Chinese (ç¹é«”ä¸­æ–‡).
        Only return the translated text, nothing else."""
        
        response = client.responses.create(
            model=model,
            instructions=instructions,
            input=text
        )
        return response.output_text
    translated_text = _ask(text, model=model)
    return {"original": text, "translated": translated_text}

    
def create_story(topic, model="gpt-5-nano", 
                 instructions="Tell the story like æ‘ä¸Šæ˜¥æ¨¹", 
                 word_count=100):
    """
    æ ¹æ“šä¸»é¡Œå‰µä½œåºŠé‚Šæ•…äº‹
    
    åƒæ•¸:
        topic (str): æ•…äº‹ä¸»é¡Œ
        model (str): ä½¿ç”¨çš„æ¨¡å‹,é è¨­ç‚º "gpt-5-nano"
        instructions (str): å¯«ä½œé¢¨æ ¼æŒ‡ç¤º,é è¨­ç‚º "Tell the story like æ‘ä¸Šæ˜¥æ¨¹"
        word_count (int): æ•…äº‹å­—æ•¸,é è¨­ç‚º 100
    
    å›å‚³:
        str: ç”Ÿæˆçš„æ•…äº‹å…§å®¹
    """
    response = client.responses.create(
        model=model,
        instructions=instructions,
        input=f"å¯«ä¸€å€‹è·Ÿ{topic}æœ‰é—œçš„åºŠé‚Šæ•…äº‹ï¼Œè¿‘{word_count}å­—çš„æ®µè½å³å¯"
    )
    return response.output_text


def task_dispatcher(user_request, model="gpt-5-nano"):
    """
    ä»»å‹™åˆ†æ´¾å™¨:æ ¹æ“šä½¿ç”¨è€…è«‹æ±‚è‡ªå‹•åˆ†æ´¾åˆ°é©ç•¶çš„å‡½å¼
    
    æ”¯æ´çš„ä»»å‹™é¡å‹:
    1. translator - ç¿»è­¯ä»»å‹™
    2. news_summarizer - æ–°èæ‘˜è¦(5W1H)
    3. story_creator - æ•…äº‹å‰µä½œ
    4. general_question - ä¸€èˆ¬å•ç­”
    
    åƒæ•¸:
        user_request (str): ä½¿ç”¨è€…çš„è«‹æ±‚å…§å®¹
        model (str): ä½¿ç”¨çš„æ¨¡å‹,é è¨­ç‚º "gpt-5-nano"
    
    å›å‚³:
        æ ¹æ“šä»»å‹™é¡å‹å›å‚³ä¸åŒæ ¼å¼çš„çµæœ
    """
    # ä½¿ç”¨ AI åˆ¤æ–·ä»»å‹™é¡å‹
    classification_prompt = f"""è«‹åˆ†æä»¥ä¸‹ä½¿ç”¨è€…è«‹æ±‚,åˆ¤æ–·æ‡‰è©²ä½¿ç”¨å“ªå€‹å‡½å¼è™•ç†ã€‚

ä½¿ç”¨è€…è«‹æ±‚: "{user_request}"

å¯ç”¨çš„å‡½å¼:
1. translator - ç”¨æ–¼ç¿»è­¯ä»»å‹™,é—œéµè©:ç¿»è­¯ã€translateã€ä¸­ç¿»è‹±ã€è‹±ç¿»ä¸­
2. news_summarizer - ç”¨æ–¼æ–°èåˆ†æå’Œæ‘˜è¦,é—œéµè©:æ–°èã€æ‘˜è¦ã€5W1Hã€åˆ†ææ–°è
3. story_creator - ç”¨æ–¼å‰µä½œæ•…äº‹,é—œéµè©:æ•…äº‹ã€åºŠé‚Šæ•…äº‹ã€å‰µä½œã€å¯«ä¸€å€‹æ•…äº‹
4. general_question - ç”¨æ–¼ä¸€èˆ¬å•ç­”,å…¶ä»–æ‰€æœ‰æƒ…æ³

è«‹åªå›ç­”å‡½å¼åç¨±,ä¸è¦æœ‰å…¶ä»–å…§å®¹ã€‚å¾ä»¥ä¸‹é¸é …ä¸­é¸ä¸€å€‹:
translator, news_summarizer, story_creator, general_question"""

    function_name = ask(
        classification_prompt,
        model=model,
        instructions="You are a task classifier. Return only the function name."
    ).strip().lower()
    
    print(f"ğŸ” åµæ¸¬åˆ°çš„ä»»å‹™é¡å‹: {function_name}")
    print(f"ğŸ“ è™•ç†è«‹æ±‚: {user_request}")
    print("-" * 60)
    
    # æ ¹æ“šåˆ¤æ–·çµæœåˆ†æ´¾ä»»å‹™
    if "translator" in function_name:
        # æå–è¦ç¿»è­¯çš„æ–‡å­—
        extract_prompt = f"å¾ä»¥ä¸‹è«‹æ±‚ä¸­æå–éœ€è¦ç¿»è­¯çš„æ–‡å­—å…§å®¹,åªå›å‚³è¦ç¿»è­¯çš„æ–‡å­—:\n{user_request}"
        text_to_translate = ask(extract_prompt, model=model, instructions="Extract only the text to translate.").strip()
        result = translator(text_to_translate, model=model)
        print(f"åŸæ–‡: {result['original']}")
        print(f"è­¯æ–‡: {result['translated']}")
        return result
    
    elif "news_summarizer" in function_name or "news" in function_name:
        # æå–æ–°èå…§å®¹
        extract_prompt = f"å¾ä»¥ä¸‹è«‹æ±‚ä¸­æå–æ–°èå…§å®¹æ–‡å­—,åªå›å‚³æ–°èæ–‡å­—æœ¬èº«:\n{user_request}"
        news_content = ask(extract_prompt, model=model, instructions="Extract only the news content.").strip()
        result = news_5w1h_summarizer(news_content, model=model)
        print("æ–°è 5W1H æ‘˜è¦:")
        print(json.dumps(result, indent=2, ensure_ascii=False))
        return result
    
    elif "story" in function_name:
        # æå–æ•…äº‹ä¸»é¡Œ
        extract_prompt = f"å¾ä»¥ä¸‹è«‹æ±‚ä¸­æå–æ•…äº‹ä¸»é¡Œ,åªå›å‚³ä¸»é¡Œé—œéµè©:\n{user_request}"
        topic = ask(extract_prompt, model=model, instructions="Extract only the story topic.").strip()
        result = create_story(topic, model=model)
        print(f"æ•…äº‹ä¸»é¡Œ: {topic}")
        print(f"æ•…äº‹å…§å®¹:\n{result}")
        return result
    
    else:  # general_question
        result = ask(user_request, model=model)
        print(f"å›ç­”: {result}")
        return result


# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    print("=" * 60)
    print("ä»»å‹™åˆ†æ´¾å™¨æ¸¬è©¦")
    print("=" * 60)
    print()
    
    # æ¸¬è©¦ 1: ç¿»è­¯ä»»å‹™
    print("ã€æ¸¬è©¦ 1ã€‘ç¿»è­¯ä»»å‹™")
    task_dispatcher("è«‹å¹«æˆ‘ç¿»è­¯: Hello, how are you today?")
    print("\n" + "=" * 60 + "\n")
    
    # æ¸¬è©¦ 2: æ–°èæ‘˜è¦ä»»å‹™
    print("ã€æ¸¬è©¦ 2ã€‘æ–°èæ‘˜è¦ä»»å‹™")
    task_dispatcher("""è«‹åˆ†æé€™å‰‡æ–°èçš„5W1H:
    å°åŒ—å¸‚é•·è”£è¬å®‰ä»Šå¤©(17æ—¥)ä¸Šåˆåœ¨å¸‚æ”¿åºœå®£å¸ƒ,å°åŒ—å¸‚å°‡åœ¨æ˜å¹´1æœˆé–‹å§‹å¯¦æ–½æ–°çš„åƒåœ¾æ¸›é‡æ”¿ç­–ã€‚
    é€™é …æ”¿ç­–æ˜¯ç‚ºäº†å› æ‡‰æ—¥ç›Šåš´é‡çš„åƒåœ¾å•é¡Œ,é€éæé«˜åƒåœ¾è™•ç†è²»å’ŒåŠ å¼·è³‡æºå›æ”¶ä¾†é”æˆæ¸›é‡ç›®æ¨™ã€‚
    å¸‚åºœé è¨ˆé€éé€™é …æªæ–½,åœ¨æœªä¾†ä¸‰å¹´å…§å°‡åƒåœ¾é‡æ¸›å°‘30%ã€‚
    """)
    print("\n" + "=" * 60 + "\n")
    
    # æ¸¬è©¦ 3: æ•…äº‹å‰µä½œä»»å‹™
    print("ã€æ¸¬è©¦ 3ã€‘æ•…äº‹å‰µä½œä»»å‹™")
    task_dispatcher("è«‹å¯«ä¸€å€‹é—œæ–¼å‹èª¼çš„åºŠé‚Šæ•…äº‹")
    print("\n" + "=" * 60 + "\n")
    
    # æ¸¬è©¦ 4: ä¸€èˆ¬å•ç­”ä»»å‹™
    print("ã€æ¸¬è©¦ 4ã€‘ä¸€èˆ¬å•ç­”ä»»å‹™")
    task_dispatcher("ä»€éº¼æ˜¯äººå·¥æ™ºæ…§?")
    print("\n" + "=" * 60 + "\n")